#! /usr/bin/python

# -*- coding: utf-8 -*-
"""Good Skiing_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DTSA5aHFMfcLiQvXfuul17x39dLmGy9c

## Package installation
"""

import random

"""## Packages import"""

import gymnasium as gym
import numpy as np
import os
from tqdm import tqdm
import pickle

"""## Environment definition"""

env = gym.make('ALE/Skiing-v5').env
env.metadata['render_fps'] = 60

"""## Environment description

### Observation space
"""

# Observation Space
env.observation_space

"""It's a Box that representes the Skiing game environment by the Atari 2600 console screen, and the observation space is a 3-dimensional numpy array of shape (210, 160, 3), the number of possible states is equal to the number of possible unique 3-dimensional arrays of shape (210, 160, 3).

### Action space
"""

env.action_space

"""It means trhat there are three possible actions in this game.

## Random actions for `env` visualisation
"""

action_size = env.action_space.n

"""### ALGO"""

# First pictures, first state
state, info = env.reset()


class SARSAAgent:
    """Here is an RL Agent using SARSA to solve our code"""

    def __init__(self, action_size, learning_rate=0.1, discount_factor=0.99, epsilon=1.0, epsilon_min=0.01,
                 epsilon_decay=0.9995):
        self.action_size = action_size
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.epsilon = epsilon
        self.epsilon_min = epsilon_min
        self.epsilon_decay = epsilon_decay
        self.all_state = []
        self.q_table = np.zeros((1, self.action_size), dtype=float)

    def choose_action(self, state_index):
        if np.random.uniform() < self.epsilon:
            action = env.action_space.sample()
        else:
            action = np.argmax(self.q_table[state_index, :])
        return action

    def learn(self, state_index, action, reward, next_state_index, next_action, done):
        part_1 = reward + self.discount_factor * self.q_table[next_state_index, next_action] - self.q_table[state_index, action]
  
        self.q_table[state_index, action] += self.learning_rate * part_1


    def decay_epsilon(self):
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

# Define hyperparameters
episodes = 300
max_steps = 50

# Define SARSA agent
agent = SARSAAgent(action_size)

# Search state in the all_state from the training
def find_array_index(arr_list, arr):
    for i, a in enumerate(arr_list):
        if np.array_equal(a, arr):
           return i

    done= True
    return 20  # Si le tableau n'a pas été trouvé dans la liste, retourne -1


if not os.path.isfile('objects.pickle'):
    print("L'un des fichiers n'existe pas. \n")

    # Train the agent using the SARSA algorithm
    for episode in tqdm(range(episodes)):
        step = 0
        state, info = env.reset()
        
        if find_array_index(agent.all_state, state)==20: 
            agent.all_state.append(state)
        #print('Le nombre de state est', len(agent.all_state))
        state_index = find_array_index(agent.all_state, state)
        #print(state_index)
        action = agent.choose_action(state_index)
        done = False

        while not done and step < max_steps:
            # Take action and observe new state and reward
            next_state, reward, done, is_truncated, info = env.step(action)
            
            if find_array_index(agent.all_state , next_state )==20: 
                agent.all_state.append(next_state)
            next_state_index = find_array_index(agent.all_state, next_state)
            # Add one line to agent q_table
            if agent.q_table.shape[0] < len(agent.all_state):
                agent.q_table = np.append(agent.q_table, [[0, 0, 0]], axis=0)

            # Choose next action based on epsilon-greedy policy
            next_action = agent.choose_action(next_state_index)
            #print(next_state_index)

            # Update Q-table
            agent.learn(state_index, action, reward, next_state_index, next_action, done)

            # Update state_index and action
            state_index = next_state_index
            action = next_action

            # Update step count
            step += 1

        # Decay epsilon
        agent.decay_epsilon()

    
    with open("objects.pickle","wb") as f:
        pickle.dump(agent.q_table,f)
        pickle.dump(agent.all_state,f)

env.close()

print("Le fichier existe déjà")
with open("objects.pickle","rb") as f:
    loaded_q_table = pickle.load(f)
    loaded_all_state = pickle.load(f)
    
env = gym.make('ALE/Skiing-v5', render_mode='human').env
# Initialize the environment
state, info = env.reset()
done = False

while not done:
    env.render()
    index = find_array_index(loaded_all_state, state)
    action = np.argmax(loaded_q_table[index, :])
    state, reward, done, is_truncated, info = env.step(action)
    
    

